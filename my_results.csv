operator,model,M,N,K,batch_size,latency_ms,tflops/tops
PyTorch-FP16-GEMM,Llama-7B,1,11008,4096,1,0.7257,0.12
PyTorch-FP16-GEMM,Llama-7B,2,11008,4096,1,0.4575,0.39
PyTorch-FP16-GEMM,Llama-7B,4,11008,4096,1,0.4565,0.79
PyTorch-FP16-GEMM,Llama-7B,8,11008,4096,1,0.4418,1.63
PyTorch-FP16-GEMM,Llama-7B,16,11008,4096,1,0.42,3.44
PyTorch-FP16-GEMM,Llama-7B,32,11008,4096,1,0.4129,6.99
PyTorch-FP16-GEMM,Llama-13B,1,13824,5120,1,0.8421,0.17
PyTorch-FP16-GEMM,Llama-13B,2,13824,5120,1,0.6218,0.46
PyTorch-FP16-GEMM,Llama-13B,4,13824,5120,1,0.6263,0.9
PyTorch-FP16-GEMM,Llama-13B,8,13824,5120,1,0.6358,1.78
PyTorch-FP16-GEMM,Llama-13B,16,13824,5120,1,0.6355,3.56
PyTorch-FP16-GEMM,Llama-13B,32,13824,5120,1,0.6827,6.64
PyTorch-FP16-GEMM,Llama-70B,1,28672,8192,1,1.8961,0.25
PyTorch-FP16-GEMM,Llama-70B,2,28672,8192,1,1.8736,0.5
PyTorch-FP16-GEMM,Llama-70B,4,28672,8192,1,1.8728,1.0
PyTorch-FP16-GEMM,Llama-70B,8,28672,8192,1,1.9045,1.97
PyTorch-FP16-GEMM,Llama-70B,16,28672,8192,1,1.9334,3.89
PyTorch-FP16-GEMM,Llama-70B,32,28672,8192,1,2.0011,7.51
PyTorch-FP32-GEMM,Llama-7B,1,11008,4096,1,0.7507,0.12
PyTorch-FP32-GEMM,Llama-7B,2,11008,4096,1,0.7246,0.25
PyTorch-FP32-GEMM,Llama-7B,4,11008,4096,1,0.7415,0.49
PyTorch-FP32-GEMM,Llama-7B,8,11008,4096,1,0.7635,0.94
PyTorch-FP32-GEMM,Llama-7B,16,11008,4096,1,0.8004,1.8
PyTorch-FP32-GEMM,Llama-7B,32,11008,4096,1,0.8519,3.39
PyTorch-FP32-GEMM,Llama-13B,1,13824,5120,1,1.138,0.12
PyTorch-FP32-GEMM,Llama-13B,2,13824,5120,1,1.1355,0.25
PyTorch-FP32-GEMM,Llama-13B,4,13824,5120,1,1.1506,0.49
PyTorch-FP32-GEMM,Llama-13B,8,13824,5120,1,1.1987,0.94
PyTorch-FP32-GEMM,Llama-13B,16,13824,5120,1,1.2972,1.75
PyTorch-FP32-GEMM,Llama-13B,32,13824,5120,1,1.3031,3.48
PyTorch-FP32-GEMM,Llama-70B,1,28672,8192,1,3.7133,0.13
PyTorch-FP32-GEMM,Llama-70B,2,28672,8192,1,3.6736,0.26
PyTorch-FP32-GEMM,Llama-70B,4,28672,8192,1,3.6707,0.51
PyTorch-FP32-GEMM,Llama-70B,8,28672,8192,1,3.7985,0.99
PyTorch-FP32-GEMM,Llama-70B,16,28672,8192,1,3.8572,1.95
PyTorch-FP32-GEMM,Llama-70B,32,28672,8192,1,4.0161,3.74
