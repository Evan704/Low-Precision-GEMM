operator,M,N,K,batch_size,latency_ms,tops
PyTorch-FP16-GEMM,4096,1024,4096,1,10.896,3.15
PyTorch-FP16-GEMM,4096,1024,4096,8,87.1926,3.15
PyTorch-FP16-GEMM,4096,1024,4096,32,282.8447,3.89
PyTorch-FP16-GEMM,2048,2048,2048,1,5.1389,3.34
PyTorch-FP16-GEMM,2048,2048,2048,16,77.1172,3.56
PyTorch-FP16-GEMM,4096,4096,128,1,5.4673,0.79
PyTorch-FP16-GEMM,4096,4096,128,32,177.3856,0.77
PyTorch-FP32-GEMM,4096,1024,4096,1,43.4709,0.79
PyTorch-FP32-GEMM,4096,1024,4096,8,179.6591,1.53
PyTorch-FP32-GEMM,4096,1024,4096,32,876.7585,1.25
PyTorch-FP32-GEMM,2048,2048,2048,1,8.9034,1.93
PyTorch-FP32-GEMM,2048,2048,2048,16,294.5596,0.93
PyTorch-FP32-GEMM,4096,4096,128,1,5.4628,0.79
PyTorch-FP32-GEMM,4096,4096,128,32,171.0342,0.8
